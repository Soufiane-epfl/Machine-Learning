{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elasticdeform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: MV Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frames(video, masks, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize video frames and corresponding masks to a given target size.\n",
    "    \"\"\"\n",
    "    resized_video = [cv2.resize(frame, target_size, interpolation=cv2.INTER_CUBIC) for frame in video]\n",
    "    resized_masks = [(cv2.resize(m.astype(np.uint8), target_size, interpolation=cv2.INTER_CUBIC)).astype(bool) for m in masks]\n",
    "    return resized_video, resized_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labeled_frames(train_data):\n",
    "    \"\"\"\n",
    "    Extract labeled frames from the training data.\n",
    "    \"\"\"\n",
    "    train_videos = []\n",
    "    train_labels = []\n",
    "    for data in train_data:\n",
    "        frames = data['frames']\n",
    "        video_frames = np.transpose(data['video'], (2,0,1))[frames]\n",
    "        label_frames = np.transpose(data['label'], (2,0,1))[frames]\n",
    "\n",
    "        train_videos.append(video_frames)\n",
    "        train_labels.append(label_frames)\n",
    "    return train_videos, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(train_videos, train_labels, target_size=(128, 128), num_augmentations=25):\n",
    "    \"\"\"\n",
    "    Resize and augment the training data directly using resize_frames and stack_transforms.\n",
    "\n",
    "    \"\"\"\n",
    "    augmented_videos = []\n",
    "    augmented_labels = []\n",
    "\n",
    "    for video, labels in zip(train_videos, train_labels):\n",
    "        # Resize original video and labels\n",
    "        resized_video, resized_label = resize_frames(video, labels, target_size)\n",
    "\n",
    "        # Add original resized set\n",
    "        augmented_videos.append(resized_video)\n",
    "        augmented_labels.append(resized_label)\n",
    "\n",
    "        # Generate augmented samples\n",
    "        for _ in range(num_augmentations):\n",
    "            aug_vid_frames = []\n",
    "            aug_lab_frames = []\n",
    "            for v_frame, l_frame in zip(resized_video, resized_label):\n",
    "                transformed_video, transformed_mask = stack_transforms(v_frame, l_frame)\n",
    "                aug_vid_frames.append(transformed_video)\n",
    "                aug_lab_frames.append(transformed_mask)\n",
    "            augmented_videos.append(aug_vid_frames)\n",
    "            augmented_labels.append(aug_lab_frames)\n",
    "\n",
    "    return augmented_videos, augmented_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_stack(augmented_videos, augmented_labels):\n",
    "    \"\"\"\n",
    "    Normalize video frames and stack them into arrays suitable for model training.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for vid, lab in zip(augmented_videos, augmented_labels):\n",
    "        for v_frame, l_frame in zip(vid, lab):\n",
    "            # Normalize frame to [0,255]\n",
    "            v_frame_norm = cv2.normalize(v_frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            X.append(np.expand_dims(v_frame_norm, 0))  # (1,H,W)\n",
    "            y.append(np.expand_dims(l_frame, 0))       # (1,H,W)\n",
    "\n",
    "    X = np.stack(X, 0) #(N, 1, H, W)\n",
    "    y = np.stack(y, 0) #(N, 1, H, W)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_deformation_transform(video, mask):\n",
    "    \"\"\"Apply elastic deformation to a single frame and mask.\"\"\"\n",
    "    displacement = np.ones((2,128,128)) * np.random.uniform(-10, 10)\n",
    "    deformed_video = elasticdeform.deform_grid(video, displacement=displacement, mode='constant', order=1)\n",
    "    deformed_mask = elasticdeform.deform_grid(mask, displacement=displacement, mode='constant', order=1)\n",
    "    return deformed_video, deformed_mask\n",
    "\n",
    "def rotation_transform(video, mask):\n",
    "    \"\"\"Apply rotation to a single frame and mask.\"\"\"\n",
    "    displacement = np.zeros((2,128,128))\n",
    "    angle = np.random.uniform(-30, 30)\n",
    "    rotated_video = elasticdeform.deform_grid(video, displacement=displacement, mode='constant', order=1, rotate=angle)\n",
    "    rotated_mask = elasticdeform.deform_grid(mask, displacement=displacement, mode='constant', order=1, rotate=angle)\n",
    "    return rotated_video, rotated_mask\n",
    "\n",
    "def zoom_transform(video, mask):\n",
    "    \"\"\"Apply zoom to a single frame and mask.\"\"\"\n",
    "    displacement = np.zeros((2,128,128))\n",
    "    zoom_factor = np.random.uniform(0.8, 1.2)\n",
    "    zoomed_video = elasticdeform.deform_grid(video, displacement=displacement, mode='constant', order=1, zoom=zoom_factor)\n",
    "    zoomed_mask = elasticdeform.deform_grid(mask, displacement=displacement, mode='constant', order=1, zoom=zoom_factor)\n",
    "    return zoomed_video, zoomed_mask\n",
    "\n",
    "def stack_transforms(video, mask):\n",
    "    \"\"\"\n",
    "    Apply a stack of transformations (elastic, rotation, zoom) sequentially to a frame and mask.\n",
    "    \"\"\"\n",
    "    deformed_video, deformed_mask = elastic_deformation_transform(video, mask)\n",
    "    rotated_video, rotated_mask = rotation_transform(deformed_video, deformed_mask)\n",
    "    zoomed_video, zoomed_mask = zoom_transform(rotated_video, rotated_mask)\n",
    "    return zoomed_video, zoomed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Preprocess and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = load_zipped_pickle(\"/content/drive/MyDrive/train.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess\n",
    "train_videos, train_labels = extract_labeled_frames(train_data)\n",
    "augmented_videos, augmented_labels = augment_data(train_videos, train_labels, target_size=(128, 128), num_augmentations=25)\n",
    "X, y = normalize_and_stack(augmented_videos, augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation Split\n",
    "test_size=0.2\n",
    "batch_size = 64\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=8, shuffle=False)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(ic, oc):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ic, oc, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(oc),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(oc, oc, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(oc),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        def upconv_block(ic, oc):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(ic, oc, kernel_size=2, stride=2),\n",
    "                nn.BatchNorm2d(oc),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        # Contracting path\n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128)\n",
    "        self.conv3 = conv_block(128, 256)\n",
    "        self.conv4 = conv_block(256, 512)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        # Expansive path\n",
    "        self.upconv4 = upconv_block(1024, 512)\n",
    "        self.conv5 = conv_block(1024, 512)\n",
    "        self.upconv3 = upconv_block(512, 256)\n",
    "        self.conv6 = conv_block(512, 256)\n",
    "        self.upconv2 = upconv_block(256, 128)\n",
    "        self.conv7 = conv_block(256, 128)\n",
    "        self.upconv1 = upconv_block(128, 64)\n",
    "        self.conv8 = conv_block(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.outconv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting path\n",
    "        c1 = self.conv1(x)\n",
    "        c2 = self.conv2(F.max_pool2d(c1, 2))\n",
    "        c3 = self.conv3(F.max_pool2d(c2, 2))\n",
    "        c4 = self.conv4(F.max_pool2d(c3, 2))\n",
    "\n",
    "        # Bottleneck\n",
    "        bn = self.bottleneck(F.max_pool2d(c4, 2))\n",
    "\n",
    "        # Expansive path\n",
    "        u4 = self.upconv4(bn)\n",
    "        c5 = self.conv5(torch.cat([c4, u4], dim=1))\n",
    "        u3 = self.upconv3(c5)\n",
    "        c6 = self.conv6(torch.cat([c3, u3], dim=1))\n",
    "        u2 = self.upconv2(c6)\n",
    "        c7 = self.conv7(torch.cat([c2, u2], dim=1))\n",
    "        u1 = self.upconv1(c7)\n",
    "        c8 = self.conv8(torch.cat([c1, u1], dim=1))\n",
    "\n",
    "        out = torch.sigmoid(self.outconv(c8))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Loss\n",
    "def jaccard_similarity(y_pred, y_true):\n",
    "    intersection = torch.sum(y_true * y_pred)\n",
    "    union = torch.sum(y_true) + torch.sum(y_pred) - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def jaccard_loss(y_pred, y_true):\n",
    "    return 1-jaccard_similarity(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "criterion = jaccard_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "num_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets) \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs_val, targets_val in val_loader:\n",
    "            inputs_val, targets_val = inputs_val.to(device), targets_val.to(device)\n",
    "            \n",
    "            val_outputs = model(inputs_val)\n",
    "            \n",
    "            val_loss = criterion(val_outputs, targets_val)#**\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Save model if performance criterion is met\n",
    "    if avg_val_loss <= 0.65:\n",
    "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/models/unet_epoch_{epoch + 1}}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint if required\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/models/val_model_checkpoint_epoch_19.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "test_data = load_zipped_pickle(\"/content/drive/MyDrive/test.pkl\")\n",
    "\n",
    "#Preprocess\n",
    "test_frames = []\n",
    "for data in test_data:\n",
    "    video = np.transpose(data['video'], (2,0,1))  # (F, H, W)\n",
    "    for frame in video:\n",
    "        # Resize\n",
    "        resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_CUBIC)\n",
    "        # Normalize\n",
    "        resized_frame = cv2.normalize(resized_frame, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        # Add channel dimension\n",
    "        test_frames.append(np.expand_dims(resized_frame, 0))  # (1,H,W)\n",
    "\n",
    "X_test = np.stack(test_frames, 0)  # (N,1,H,W)\n",
    "\n",
    "test_dataset = TensorDataset(X_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold= 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        preds = model(inputs)\n",
    "        for p in preds:\n",
    "            # Threshold\n",
    "            mask = (p[0] >= threshhold).cpu().numpy()\n",
    "            predictions.append(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_morphological_closing(predictions):\n",
    "    \"\"\"\n",
    "    Apply morphological closing to a list of boolean predictions.\n",
    "    \"\"\"\n",
    "    post_processed = []\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "\n",
    "    for p in predictions:\n",
    "        # Convert boolean mask to uint8 for OpenCV\n",
    "        p_uint8 = (p.astype(np.uint8) * 255)\n",
    "        # Apply morphological closing\n",
    "        closed_mask = cv2.morphologyEx(p_uint8, cv2.MORPH_CLOSE, kernel)\n",
    "        # Convert back to boolean\n",
    "        closed_bool = (closed_mask > 0)\n",
    "        post_processed.append(closed_bool)\n",
    "\n",
    "    return post_processed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_predictions_to_original(post_processed_predictions, test_data):\n",
    "    \"\"\"\n",
    "    Resize the post-processed predictions back to the original video resolutions.\n",
    "\n",
    "    \"\"\"\n",
    "    final = []\n",
    "    current_idx = 0\n",
    "    for data in test_data:\n",
    "        original_video = data['video']\n",
    "        h, w, num_frames = original_video.shape\n",
    "\n",
    "        for f in range(num_frames):\n",
    "            pred_mask = post_processed_predictions[current_idx].astype(np.uint8)\n",
    "            pred_resized = cv2.resize(pred_mask, (w, h), interpolation=cv2.INTER_AREA).astype(bool)\n",
    "            final.append(pred_resized)\n",
    "            current_idx += 1\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post process\n",
    "post_processed_predictions = apply_morphological_closing(predictions)\n",
    "resized_post_processed_predictions = resize_predictions_to_original(post_processed_predictions, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prediction in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sequences(arr):\n",
    "    \"\"\"\n",
    "    Extract sequences of contiguous 1s in a binary flattened array.\n",
    "    Returns start indices and lengths of each sequence of ones.\n",
    "    \"\"\"\n",
    "    arr = arr.astype(int)\n",
    "    first_indices = []\n",
    "    last_indices = []\n",
    "    extended_arr = np.concatenate(([0], arr, [0]))\n",
    "    \n",
    "    for i in range(len(extended_arr) - 1):\n",
    "        # A run starts when we encounter a transition from 0 to 1\n",
    "        if extended_arr[i] == 0 and extended_arr[i+1] == 1:\n",
    "            first_indices.append(i)\n",
    "        # A run ends when we encounter a transition from 1 to 0\n",
    "        if extended_arr[i] == 1 and extended_arr[i+1] == 0:\n",
    "            last_indices.append(i)\n",
    "\n",
    "    lengths = [l - f for f, l in zip(first_indices, last_indices)]\n",
    "    return first_indices, lengths\n",
    "\n",
    "\n",
    "def create_submission_from_final(final_masks, test_data, output_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Create the submission file from the final predicted masks.\n",
    "    \"\"\"\n",
    "    ids, values = [], []\n",
    "    count = 0\n",
    "    current_idx = 0\n",
    "\n",
    "    # Process each video at once\n",
    "    for data in test_data:\n",
    "        video_name = data['name']\n",
    "        h, w, f = data['video'].shape  # original video shape (H, W, frames)\n",
    "\n",
    "        # Gather all frames for this video\n",
    "        video_masks = final_masks[current_idx:current_idx + f]\n",
    "        current_idx += f\n",
    "\n",
    "        # Stack them to form a 3D mask of shape (H, W, F)\n",
    "        video_mask_3d = np.stack(video_masks, axis=-1)  # (H, W, F)\n",
    "\n",
    "        # Flatten the 3D mask\n",
    "        flat_mask = video_mask_3d.flatten().astype(int)\n",
    "\n",
    "        # Find runs of consecutive 1s in the flattened mask\n",
    "        start_indices, lengths = get_sequences(flat_mask)\n",
    "\n",
    "        # Add each run to the CSV rows\n",
    "        for start_idx, length in zip(start_indices, lengths):\n",
    "            count += 1\n",
    "            unique_id = f\"{video_name}_{count}\"\n",
    "            value_str = f\"[{start_idx}, {length}]\"\n",
    "            ids.append(unique_id)\n",
    "            values.append(value_str)\n",
    "\n",
    "    # Create and save submission dataframe\n",
    "    df = pd.DataFrame({\"id\": ids, \"value\": values})\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save submission\n",
    "create_submission_from_final(resized_post_processed_predictions, test_data, output_file=\"/content/drive/MyDrive/submission_files/submission_rightformat.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *******************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
